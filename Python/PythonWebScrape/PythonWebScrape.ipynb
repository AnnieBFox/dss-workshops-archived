{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Web scraping with Python\"\n",
    "always_allow_html: yes\n",
    "output: \n",
    "  html_document:\n",
    "    highlight: tango\n",
    "    toc: true\n",
    "    toc_float:\n",
    "      collapsed: true       \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "pre code {\n",
    "    display: block;\n",
    "    unicode-bidi: embed;\n",
    "    font-family: monospace;\n",
    "    white-space: pre;\n",
    "    max-height: 400px;\n",
    "    overflow-x: scroll;\n",
    "    overflow-y: scroll;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "results": "'hide'"
   },
   "outputs": [],
   "source": [
    "## This part is optional; it sets some printing options\n",
    "## that make output look nicer.\n",
    "from pprint import pprint as print\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 133)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_columns', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup instructions\n",
    "\n",
    "###  Install the Anaconda Python distribution\n",
    "If using your own computer please install the Anaconda Python\n",
    "distribution from\n",
    "[https://www.anaconda.com/download/](https://www.anaconda.com/download/).\n",
    "(Note that Python version$\\leq$ 3.0 differs considerably from more\n",
    "recent releases. For this workshop you will need version$\\geq$ 3.4.)\n",
    "\n",
    "Accepting the defaults proposed by the Anaconda installer is generally\n",
    "recommended. However, if it offers to install Microsoft Visual Studio\n",
    "Code you may safely skip this step.\n",
    "\n",
    "### Download workshop materials\n",
    "Download the materials from\n",
    "[http://tutorials.iq.harvard.edu/Python/PythonWebScrape.zip](http://tutorials.iq.harvard.edu/Python/PythonWebScrape.zip)\n",
    "and extract the zipped directory (Right-click => Extract All on\n",
    "Windows, double-click on Mac).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop goals and approach\n",
    "In this workshop you will\n",
    "- learn basic web scraping principles and techniques,\n",
    "- learn how to use the requests package in Python,\n",
    "- practice making requests and manipulating responses from the server.\n",
    "\n",
    "This workshop is relatively *informal*, *example-oriented*, and\n",
    "*hands-on*. We will learn by working through an example web scraping\n",
    "project.\n",
    "\n",
    "Note that this is **not** an introductory workshop. Familiarity with\n",
    "Python, including but not limited to knowledge of lists and\n",
    "dictionaries, indexing, and loops and / or comprehensions is assumed.\n",
    "If you need an introduction to Python or a refresher, we recommend the\n",
    "[IQSS Introduction to Python](https://dss.iq.harvard.edu/workshop-materials#widget-0).\n",
    "\n",
    "Note also that this workshop will not teach you everything you need to\n",
    "know in order to retrieve any data whatsoever from any webs service\n",
    "whatsoever. This workshop will give you just enough to get started.\n",
    "\n",
    "## Preliminary questions\n",
    "\n",
    "### What is web scraping?\n",
    "Web scraping the activity of automating retrieval of information from\n",
    "a web service designed for human interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is web scraping legal? Is it ethical?\n",
    "It depends. If you have legal questions seek legal counsel. You can\n",
    "mitigate some ethical issues by building delays and restrictions into\n",
    "your web scraping program so as to avoid impacting the availability of\n",
    "the web service for other users or the cost of hosting the service for\n",
    "the service provider.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example project overview and goals\n",
    "In this workshop I will demonstrate web scraping techniques using the\n",
    "Exhibitions page at\n",
    "<https://www.harvardartmuseums.org/visit/exhibitions> and let you use\n",
    "the skills you'll learn to retrieve information from other parts of\n",
    "the Harvard Art Museums website.\n",
    "\n",
    "The basic strategy is pretty much the same for most scraping projects.\n",
    "We will use our web browser (Chrome or Firefox recommended) to examin\n",
    "the page you wish to retrieve data from, and copy/paste information\n",
    "from your web browser into your scraping program.\n",
    "\n",
    "## Take shortcuts if you can\n",
    "We wish to extract information from\n",
    "<https://www.harvardartmuseums.org/visit/exhibitions>. Like most\n",
    "modern web pages, a lot goes on behind the scenes to produce the page\n",
    "we see in our browser. Our goal is to pull back the curtain to see\n",
    "what the website does when we interact with it. Once we see how the\n",
    "website works we can start retrieving data from it. If we are lucky\n",
    "we'll find a resource that returns the data we're looking for in a\n",
    "structured format like [JSON](https://json.org/) or\n",
    "[XML](https://en.wikipedia.org/wiki/XML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the structure of our target web service\n",
    "\n",
    "We start by opening that page in a web browser and inspecting it.\n",
    "\n",
    "![](img/dev_tools.png)\n",
    "\n",
    "![](img/dev_tools_pane.png)\n",
    "\n",
    "If we scroll down to the bottom of the Exhibitions page, we'll see a\n",
    "button that says \"Load More Exhibitions\". Let's see what happens when\n",
    "we click on that button.To do so, click on \"Network\" in the developer\n",
    "tools window,then click the button. You should see a list of requests\n",
    "that were made as a result of clicking that button, as shown below.\n",
    "\n",
    "![](img/dev_tools_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at that second request, the one that to `load_next`, we'll\n",
    "see that it returns all the information we need, in a convenient\n",
    "format called `JSON`. All we need to retrieve exhibition data is call\n",
    "make `GET` requests to\n",
    "<https://www.harvardartmuseums.org/search/load_next> with the correct\n",
    "parameters. \n",
    "\n",
    "### Making requests using python\n",
    "The URL we want to retrieve data from has the following structure\n",
    "\n",
    "    scheme                    domain              path                  parameters\n",
    "     https www.harvardartmuseums.org  search/load_next type=past-exhibition&page=0\n",
    "\n",
    "It is often convenient to create variables containing the domain(s)\n",
    "and path(s) you'll be working with, as this allows you to swap out\n",
    "paths and parameters as needed. Note that the path is separated from\n",
    "the domain with `/` and the parameters are separated from the path\n",
    "with `?`. If there are multiple parameters they are separated from\n",
    "each other with a `&`.\n",
    "\n",
    "For example, we can retrieve the first set of exhibitions in Python as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_domain = 'https://www.harvardartmuseums.org'\n",
    "exhibition_path = 'search/load_next'\n",
    "exhibition_parameters = 'type=past-exhibition'\n",
    "\n",
    "exhibition_url = (museum_domain\n",
    "                  + \"/\"\n",
    "                  + exhibition_path\n",
    "                  + \"?\"\n",
    "                  + exhibition_parameters)\n",
    "\n",
    "print(exhibition_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've constructed the URL we wish interact with we're ready\n",
    "to make our first request in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "exhibitions0 = requests.get(exhibition_url \n",
    "                            + '&'\n",
    "                            + 'page=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Parsing JSON data\n",
    "We already know from inspecting network traffic in our web\n",
    "browser that this URL returns JSON, but we can use Python to verify\n",
    "this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibitions0.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since JSON is a structured data format, parsing it into python data\n",
    "structures is easy. In fact, there's a method for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibitions0 = exhibitions0.json()\n",
    "print(exhibitions0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Really, we are done here. Everyone go home!\n",
    "\n",
    "OK not really, there is still more we can lean. But you have to admit\n",
    "that was pretty easy. If you can identify a service that returns the\n",
    "data you want in structured from, web scraping becomes a pretty\n",
    "trivial enterprise. We'll discuss several other scenarios and topics,\n",
    "but for some web scraping tasks this is really all you need to know.\n",
    "\n",
    "### Organizing and saving the data\n",
    "\n",
    "The records we retrieved from\n",
    "`https://www.harvardartmuseums.org/museum_exhibitions/search/load_next`\n",
    "are arranged as a list of dictionaries. With only a little trouble we\n",
    "can select the fields of interest and arrange these data into a pandas\n",
    "`DataFrame`. First lets see what fields are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exhibitions0.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exhibitions0['records'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can specify the fields we are interested in and use a dict\n",
    "comprehension to organize the values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fields_to_keep = ['title', 'poster', 'people',\n",
    "                  'begindate', 'enddate']\n",
    "\n",
    "records0 = {k: [record.get(k, 'NA')\n",
    "                for record in exhibitions0['records']]\n",
    "            for k in fields_to_keep}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can convert the dict to a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records0 = pd.DataFrame.from_dict(records0)\n",
    "\n",
    "print(records0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and write the data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records0.to_csv(\"records0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating to retrieve all the data\n",
    "\n",
    "Of course we don't want just the first page of exhibitions. How can we\n",
    "retrieve all of them?\n",
    "\n",
    "The first page of JSON data we retrieved contains meta data about the\n",
    "number of records, in the `info` field. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exhibitions0['info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how many pages there are we can iterate over them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [requests.get(exhibition_url\n",
    "                        + '&'\n",
    "                        + 'page='\n",
    "                        + str(page)).json()['records']\n",
    "           for page in range(1, exhibitions0['info']['pages'] + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we can flatten the records in each list into one long\n",
    "`records` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_final = []\n",
    "for record in records:\n",
    "    records_final += record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can write the data to a .csv file without too much\n",
    "difficulty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_final = {k: [record.get(k, 'NA')\n",
    "                     for record in records_final]\n",
    "                 for k in fields_to_keep}\n",
    "\n",
    "## convert the dict to a `DataFrame`\n",
    "records_final = pd.DataFrame.from_dict(records_final)\n",
    "\n",
    "# write the data to a file.\n",
    "records_final.to_csv(\"records_final.csv\")\n",
    "\n",
    "print(records_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Retrieve exhibits data\n",
    "In this exercise you will retrieve information about the art\n",
    "collections at Harvard Art Museums from\n",
    "`https://www.harvardartmuseums.org/collections`\n",
    "\n",
    "1. Using a web browser (Firefox or Chrome recommended) inspect the\n",
    "   page at `https://www.harvardartmuseums.org/collections`. Examine\n",
    "   the network traffic as you interact with the page. Try to find\n",
    "   where the data displayed on that page comes from.\n",
    "2. Make a `get` request in Python to retrieve the data from the URL\n",
    "   identified in step1.\n",
    "3. Write a *loop* or *list comprehension* in Python to retrieve data\n",
    "   for the first 5 pages of collections data.\n",
    "4. Bonus (optional): Arrange the data you retrieved into dict of\n",
    "   lists. Convert it to a pandas `DataFrame` and save it to a `.csv`\n",
    "   file.\n",
    "   \n",
    "## Parse html if you have to\n",
    "As we've seen, you can often inspect network traffic or other sources\n",
    "to locate the source of the data you are interested in and the API\n",
    "used to retrieve it. You should always start by looking for these\n",
    "shortcuts and using them where possible. If you are really lucky,\n",
    "you'll find a shortcut that returns the data as JSON or XML. If you\n",
    "are not quite so lucky, you will have to parse HTML to retrieve the\n",
    "information you need.\n",
    "\n",
    "For example, when I inspected the network traffic while interacting\n",
    "with <https://www.harvardartmuseums.org/visit/calendar> I didn't see\n",
    "any requests that returned JSON data. The best we can do appears to be\n",
    "<https://www.harvardartmuseums.org/visit/calendar?type=&date=>, which\n",
    "unfortunately returns  HTML. \n",
    "\n",
    "### Retrieving HTML\n",
    "The first step is the same as before: we make at `GET` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_path = 'visit/calendar'\n",
    "\n",
    "calendar_url = (museum_domain # recall that we defined museum_domain earlier\n",
    "                  + \"/\"\n",
    "                  + calendar_path)\n",
    "\n",
    "print(calendar_url)\n",
    "\n",
    "events0 = requests.get(calendar_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can check the headers to see what type of content we\n",
    "received in response to our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events0.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML using the lxml library\n",
    "Like JSON, HTML is structured; unlike JSON it is designed to be\n",
    "rendered into a human-readable page rather than simply to store and\n",
    "exchange data in a computer-readable format. Consequently, parsing\n",
    "HTML and extracting information from it is somewhat more difficult\n",
    "than parsing JSON.\n",
    "\n",
    "While JSON parsing is built into the Python `requests` library, parsing\n",
    "HTML requires a separate library. I recommend using the HTML parser\n",
    "from the `lxml` library; others prefer an alternative called\n",
    "`BeautyfulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "events_html = html.fromstring(events0.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using xpath to extract content from HTML\n",
    "`XPath` is a tool for identifying particular elements withing a HTML\n",
    "document. The developer tools built into modern web browsers make it\n",
    "easy to generate `XPath`s that can used to identify the elements of a\n",
    "web page that we wish to extract.\n",
    "\n",
    "We can open the html document we retrieved and inspect it using\n",
    "our web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "'hide'"
   },
   "outputs": [],
   "source": [
    "html.open_in_browser(events_html, encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/dev_tools_right_click.png)\n",
    "\n",
    "![](img/dev_tools_inspect.png)\n",
    "\n",
    "Once we identify the element containing the information of interest we\n",
    "can use our web browser to copy the `XPath` that uniquely identifies\n",
    "that element.\n",
    "\n",
    "![](img/dev_tools_xpath.png)\n",
    "\n",
    "Next we can use python to extract the element of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list_html = events_html.xpath('//*[@id=\"events_list\"]')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can use a web browser to inspect the HTML we're\n",
    "currently working with, and to figure out what we want to extract from\n",
    "it. Let's look at the first element in our events list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "'hide'"
   },
   "outputs": [],
   "source": [
    "first_event_html = events_list_html[0]\n",
    "html.open_in_browser(first_event_html, encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can use our browser to find the xpath of the elements we\n",
    "want.\n",
    "\n",
    "![](img/dev_tools_figcaption.png)\n",
    "\n",
    "(Note that the `html.open_in_browser` function adds enclosing `html`\n",
    "and `body` tags in order to create a complete web page for viewing.\n",
    "This requires that we adjust the `xpath` accordingly.)\n",
    "\n",
    "By repeating this process for each element we want, we can build a\n",
    "list of the xpaths to those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_we_want = {'figcaption': 'div/figure/div/figcaption',\n",
    "                    'date': 'div/div/header/time',\n",
    "                    'title': 'div/div/header/h2/a',\n",
    "                    'time': 'div/div/div/p[1]/time',\n",
    "                    'localtion1': 'div/div/div/p[2]/span/span[1]',\n",
    "                    'location2': 'div/div/div/p[2]/span/span[2]'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can iterate over the elements we want and extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_event_values = {key: first_event_html.xpath(\n",
    "    elements_we_want[key])[0].text_content().strip()\n",
    "                      for key in elements_we_want.keys()}\n",
    "\n",
    "print(first_event_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating to retrieve content from a list of HTML elements\n",
    "So far we've retrieved information only for the first event. To\n",
    "retrieve data for all the events listed on the page we need to iterate\n",
    "over the events. If we are very lucky, each event will have exactly\n",
    "the same information structured in exactly the same way and we can\n",
    "simply extend the code we wrote above to iterate over the events list.\n",
    "\n",
    "Unfortunately not all these elements are available for every event, so\n",
    "we need to take care to handle the case where one or more of these\n",
    "elements is not available. We can do that by defining a function that\n",
    "tries to retrieve a value and returns an empty string if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_info(event, path):\n",
    "    try:\n",
    "        info = event.xpath(path)[0].text.strip()\n",
    "    except:\n",
    "        info = ''\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this function we can iterate over the list of events and\n",
    "extract the available information for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_event_values = {key: [get_event_info(event, elements_we_want[key])\n",
    "                        for event in events_list_html]\n",
    "                    for key in elements_we_want.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we can arrange these values in a pandas `DataFrame`\n",
    "and save them as .csv files, just as we did with our exhibitions data earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_event_values = pd.DataFrame.from_dict(all_event_values)\n",
    "\n",
    "all_event_values.to_csv(\"all_event_values.csv\")\n",
    "\n",
    "print(all_event_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: parsing HTML\n",
    "In this exercise you will retrieve information about the physical\n",
    "layout of the Harvard Art Museums. The web page at\n",
    "<https://www.harvardartmuseums.org/visit/floor-plan> contains this\n",
    "information in HTML from.\n",
    "\n",
    "1. Using a web browser (Firefox or Chrome recommended) inspect the\n",
    "   page at `https://www.harvardartmuseums.org/visit/floor-plan`. Copy\n",
    "   the `XPath` to the element containing the list of level\n",
    "   information. (HINT: the element if interest is a `ul`, i.e.,\n",
    "   `unordered list`.)\n",
    "2. Make a `get` request in Python to retrieve the web page at\n",
    "   <https://www.harvardartmuseums.org/visit/floor-plan>. Extract the\n",
    "   content from your request object and parse it using `html.fromstring`\n",
    "   from the `lxml` library.\n",
    "3. Use your web browser to find the `XPath`s to the facilities housed on\n",
    "   level one. Use Python to extract the text from those `Xpath`s.\n",
    "4. Bonus (optional): Write a *loop* or *list comprehension* in Python\n",
    "   to retrieve data for all the levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a browser driver as a last resort\n",
    "\n",
    "TODO\n",
    "\n",
    "## Use Scrapy for large or complicated projects\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "jupytext_formats": "ipynb,Rmd:rmarkdown,py,md:markdown",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "name": "PythonWebScrape.ipynb",
  "toc": {
   "base_numbering": "1",
   "nav_menu": {
    "height": "542px",
    "width": "461px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
