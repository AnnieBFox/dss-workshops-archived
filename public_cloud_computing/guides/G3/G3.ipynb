{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide 3: Public Cloud Computing with Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Public Cloud Computing is (**TODO** explain).  \n",
    "It is somehow expensive (**TODO** explain).\n",
    "You can communicate how (**TODO** explain). \n",
    "\n",
    "What is cloud computing\n",
    "\n",
    "In this tutorial we are going to explain: a), b) , c)\n",
    "\n",
    "We'll go over the basics of Microsoft Azure Storage, but we should point out that a *lot* of talented people have given tutorials, and we won't do any better than they have. \n",
    "*TODO Point out some resources and explain why they are good (add links)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Guide 3: Public Cloud Computing with Microsoft Azure](#Guide-2:-Public-Cloud-Computing-with-Microsoft-Azure)\n",
    "    * [Cloud computing](#Cloud-storage-and-cloud-computing)\n",
    "    * [Azure cloud computing basics](#Azure-Storage-Account-basics)\n",
    "    * [Common Tasks in using Azure Cloud Computing services](#Common-Tasks-in-using-Azure-Storage-Account)\n",
    "        * [Use API](#Paragraph-3)\n",
    "        * [Request token](#Paragraph-3)\n",
    "    * [Demo 1: Detect Face Features and Emotion](#Paragraph-3)\n",
    "    * [Demo 2: Analyze Image](#Paragraph-3)\n",
    "    * [Demo 3: Optical Character Recognition](#Paragraph-3)\n",
    "    * [Demo 4: Speech Recognition](#Paragraph-3)\n",
    "    * [Demo 5: Key Phrases](#Paragraph-3)\n",
    "    * [Demo 6: Sentiment Analysis](#Paragraph-3)\n",
    "    * [Demo 7: Bing Search](#Paragraph-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure cloud computing basics\n",
    "## Common Tasks in using Azure Cloud Computing services\n",
    "\n",
    "### Use API\n",
    "### Request token\n",
    "\n",
    "## Demo 1: Detect Face Features and Emotion\n",
    "## Demo 2: Analyze Image\n",
    "## Demo 3: Optical Character Recognition\n",
    "## Demo 4: Speech Recognition\n",
    "## Demo 5: Key Phrases\n",
    "## Demo 6: Sentiment Analysis\n",
    "## Demo 7: Bing Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azure library import methods to use storage \n",
    "from azure.storage.blob import BlockBlobService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the storage account name \n",
    "azure_storage_account_name = 'cloudcomputingplayground' # add name\n",
    "\n",
    "#set the storage account key\n",
    "azure_storage_account_key = None\n",
    "\n",
    "#set the container name (the foder with the files)\n",
    "blob_container_name = 'cloudcomputingcontainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create blob service object to access the files in the storage*\n",
    "blob_service = BlockBlobService(azure_storage_account_name, azure_storage_account_key)\n",
    "\n",
    "# create an object with a list a list of files associated with the container\n",
    "blobs_container_file_list = blob_service.list_blobs(blob_container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of files in the container:\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Eisenhower_1952.chunk0.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Eisenhower_1952.chunk1.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Eisenhower_1952.chunk2.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Eisenhower_1952.chunk3.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Weekly_Address_61618_1.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Weekly_Address_61618_2.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Weekly_Address_61618_3.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Weekly_Address_61618_4.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/Weekly_Address_61618_5.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/andrea_porelli.jpg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/clinton.txt\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/cloud_providers_queries_gt.csv\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/debate.txt\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/emotion1.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/emotion2.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/emotion3.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/face1.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/face2.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/face3.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/face4.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/ista_zahn.png\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/ocr1.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/ocr2.jpeg\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/ocr_iqss_1.png\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/speech1.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/speech2.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/speech3.wav\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/trump.txt\n",
      "https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/vision1.jpeg\n"
     ]
    }
   ],
   "source": [
    "# set url format\n",
    "blob_prefix = 'https://{0}.blob.core.windows.net/{1}/{2}'\n",
    "\n",
    "# print the list of files\n",
    "print(\"List of files in the container:\")\n",
    "for blob in blobs_container_file_list:\n",
    "    print(blob_prefix.format(blob_service.account_name, blob_container_name, blob.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1) Detect Face Features and Emotion (face/v1.0/detect)\n",
    "\n",
    "_**Description:**_\n",
    "Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes. Attributes include age, gender, headPose, smile, facialHair, glasses, emotion, hair, makeup, occlusion, accessories, blur, exposure and noise. It can detects up to 64 faces for an image. Faces are ranked by face rectangle size from large to small. Face detector prefer frontal and near-frontal faces. There are cases that faces may not be detected, e.g. exceptionally large face angles (head-pose) or being occluded, or wrong image orientation.\n",
    "\n",
    "_**Input/Output:**_\n",
    "JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n",
    "Faces are detectable when its size is 36x36 to 4096x4096 pixels. If need to detect very small but clear faces, please try to enlarge the input image. Higher face image quality means better detection and recognition precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n",
    "\n",
    "A succesfull response returns:\n",
    "![face_detect_output](img/face_detect_output.PNG)\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation ([FACE doc](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236))\n",
    "\n",
    "### Show case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create container\n",
    "\n",
    "###YOUR CODE HERE\n",
    "\n",
    "#upload image\n",
    "\n",
    "###YOUR CODE HERE\n",
    "\n",
    "#display image\n",
    "\n",
    "###YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries to display image\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FACE API service Url (change based on the region*)\n",
    "face_api = 'https://eastus.api.cognitive.microsoft.com/face/v1.0/detect'\n",
    "# Azure Cognitive Services Key for FACE API\n",
    "key_1 = '620facc4717d4136b665e43a618c2911'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/face1.jpeg'    \n",
    "img = ipImage(url=img_url, width=300, height=350)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json', \n",
    "           'Ocp-Apim-Subscription-Key':key_1}\n",
    "\n",
    "params = urllib.parse.urlencode({'returnFaceId' : 'true',\n",
    "                                 'returnFaceLandmarks' : 'true',\n",
    "                                 'returnFaceAttributes' : 'age,gender,emotion,smile,hair,exposure'})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = face_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image's name to read\n",
    "blob_name = 'face1.jpeg'\n",
    "\n",
    "# call method (i.e. get_blob_to_bytes to) read the images as byte array\n",
    "blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "# set variable that store in memory image's bytes \n",
    "blob_bytes_in_memory = io.BytesIO(blob.content)\n",
    "\n",
    "# read bytes and display image in the notebook\n",
    "img_bytes = Image.open(blob_bytes_in_memory)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(img_bytes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries to send HTTP requests and to craft URL\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the request headers\n",
    "headers = {'Content-Type': 'application/octer-stream', # or 'application/json' for remote stored image files\n",
    "           'Ocp-Apim-Subscription-Key': key_1} # for locally stored\n",
    "\n",
    "# Set request parameters converted to HTTP protocol\n",
    "# these parameters tell the api I want to detect a face and a smile\n",
    "params = urllib.parse.urlencode({'returnFaceId' : 'true',\n",
    "                                 'returnFaceLandmarks' : 'true',\n",
    "                                 'returnFaceAttributes' : 'age,gender'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compone query with chosen parameters and associaed URL\n",
    "query = '?{0}'.format(params)\n",
    "url = face_api + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FACE api only returns 1 analysis at time\n",
    "api_response = requests.post(url, \n",
    "                             headers=headers, \n",
    "                             data=blob.content) # use local memory (use content of blob converted to array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How ML helps finding faces?\n",
    "\n",
    "##################################################################\n",
    "#                                                                #\n",
    "#                                                                #\n",
    "#                                                                #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in res_json:\n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "\n",
    "    pc = 'red' # patch color\n",
    "    if i['faceAttributes']['gender'] == 'male':\n",
    "        pc = 'blue'\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    ax.text(fr['left'], fr['top']+fr['height'], \n",
    "            'age:'+str(i['faceAttributes']['age']), \n",
    "            fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "\n",
    "# ps = 5       # patch size\n",
    "# pc = '#00FF00'   # patch color\n",
    "\n",
    "# #Draw eye, nose, mouth\n",
    "# for i in res_json:\n",
    "#     fl = i['faceLandmarks']\n",
    "\n",
    "#     # left eye\n",
    "#     ax.add_patch(patches.Circle((fl['pupilLeft']['x'], fl['pupilLeft']['y']), ps, color=pc))\n",
    "\n",
    "#     # right eye\n",
    "#     ax.add_patch(patches.Circle((fl['pupilRight']['x'], fl['pupilRight']['y']), ps, color=pc))\n",
    "\n",
    "#     # mouth\n",
    "#     ax.add_patch(patches.Circle((fl['mouthLeft']['x'], fl['mouthLeft']['y']), ps, color=pc))\n",
    "#     ax.add_patch(patches.Circle((fl['mouthRight']['x'], fl['mouthRight']['y']), ps, color=pc))\n",
    "\n",
    "#     # nose\n",
    "#     ax.add_patch(patches.Circle((fl['noseTip']['x'], fl['noseTip']['y']), ps, color=pc))\n",
    "\n",
    "plt.imshow(img_bytes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2) Analyze Image (vision/v1.0/analyze)\n",
    "\n",
    "\n",
    "_**Description:**_\n",
    "- Blah,blah\n",
    "\n",
    "_**Input/Output:**_\n",
    "- This, and that\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation (show the example on the user, then on the tutorial): https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa\n",
    "\n",
    "**##TODO** add link in pythonic, format (code style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import library to display images\n",
    "from IPython.display import Image as ipImage, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process:\n",
    "#1_Set_Cognitive_Service_Key(e.g. Face, Computer Vision, etc)\n",
    "#2_Set_Service_API(eg. from FACE choose DETECT, from COMPUTER VISION choose ANALYZE)\n",
    "#3_BlOB\n",
    "#4_Display_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Azure Cognitive Services Key for COMPUTER VISION API\n",
    "azure_key_computer_vision = 'b9fb47b349694da0bb42ffb5d687c7c0'\n",
    "\n",
    "# Set API for Analyze service\n",
    "analyze_api = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/analyze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "img_url = 'https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/vision1.jpeg'    \n",
    "img = ipImage(url=img_url, width=450, height=450)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json', \n",
    "           'Ocp-Apim-Subscription-Key':azure_key_computer_vision}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'visualFeatures': 'Categories,Tags,Description,Faces,ImageType,Color,Adult',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = analyze_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**##TODO** \n",
    " - find more interesting pictures\n",
    " - display nice output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_Demo 3) Optical Character Recognition (vision/v1.0/ocr)_**\n",
    "\n",
    "\n",
    "_**Description:**_\n",
    "- Blah,blah\n",
    "\n",
    "_**Input/Output:**_\n",
    "- This, and that\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation (show the example on the user, then on the tutorial): \n",
    "https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fc\n",
    "\n",
    "**##TODO** add link in pythonic, format (code style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Azure Cognitive Services Key for COMPUTER VISION API\n",
    "# azure_key_computer_vision = 'b9fb47b349694da0bb42ffb5d687c7c0' (do not need this again)\n",
    "\n",
    "# Set API for OCR service\n",
    "ocr_api = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "# blob_name = 'ocr_iqss_1.png'\n",
    "# blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "# image_file_in_mem = io.BytesIO(blob)\n",
    "# img_bytes = Image.open(image_file_in_mem)\n",
    "\n",
    "# set image's name to read\n",
    "blob_name = 'ocr_iqss_1.png'\n",
    "\n",
    "# call method (i.e. get_blob_to_bytes to) read the images as byte array\n",
    "blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "# set variable that store in memory image's bytes \n",
    "blob_bytes_in_memory = io.BytesIO(blob.content)\n",
    "\n",
    "# read bytes and display image in the notebook\n",
    "img_bytes = Image.open(blob_bytes_in_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "# img_url = 'https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/ocr_iqss_1.png'\n",
    "# img = ipImage(url=img_url, width=500, height=500)\n",
    "# display(img)\n",
    "\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyse_api = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr'\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', \n",
    "           'Ocp-Apim-Subscription-Key':azure_key_computer_vision}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'language': 'unk',\n",
    "    'detectOrientation': 'true',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = ocr_api + query_string\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "text_angle = 0\n",
    "try:\n",
    "    text_angle = res_json['textAngle']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Draw OCR rectangles\n",
    "for i in res_json['regions']:\n",
    "    pl = i['lines']\n",
    "    for k in pl:\n",
    "        words = k['words']\n",
    "        for l in words:\n",
    "            bb = l['boundingBox']\n",
    "            txt = l['text']\n",
    "            \n",
    "            bb = list(map(int, bb.split(',')))\n",
    "            \n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (bb[0], bb[1]), bb[2], bb[3], angle=text_angle,\n",
    "                    fill=False, linewidth=4, color='#00FF00')\n",
    "            )\n",
    "            \n",
    "            ax.text(bb[0], bb[1], txt, \n",
    "                    fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_Demo 4) Speech Recognition (sts/v1.0/issueToken)_**\n",
    "\n",
    "\n",
    "_**Description:**_\n",
    "- Blah,blah\n",
    "\n",
    "_**Input/Output:**_\n",
    "- This, and that\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation (show the example on the user, then on the tutorial): \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/speech/getstarted/getstartedrest?tabs=Powershell\n",
    "\n",
    "**##TODO** \n",
    "add link in pythonic, \n",
    "format (code style), \n",
    "add picture to explain how it works(if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make an example simple and build the more complex\n",
    "\n",
    "azure_key_bing_speech = '5d14da803af54010ba390d560b203354'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import requests\n",
    "import urllib\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_list = ['Eisenhower_1952.chunk0.wav','Eisenhower_1952.chunk1.wav'\n",
    "             'Eisenhower_1952.chunk2.wav','Eisenhower_1952.chunk3.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# load speech file to process\n",
    "blob_name = 'Eisenhower_1952.chunk0.wav'\n",
    "blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "wav_bytes = Audio(data=blob.content)\n",
    "display(wav_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uri_token = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken'\n",
    "\n",
    "headers = {'Content-Length': '0', \n",
    "           'Ocp-Apim-Subscription-Key': azure_key_bing_speech}\n",
    "\n",
    "api_response = requests.post(uri_token, headers=headers)\n",
    "\n",
    "access_token = str(api_response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'RIFFD((\\x00WAVEfmt \\x10\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.content[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Service\n",
    "# Call Speech to text service\n",
    "url_stt_api = 'https://speech.platform.bing.com/recognize' # service address \n",
    "\n",
    "headers = {\n",
    "           'Authorization': 'Bearer {0}'.format(access_token),\n",
    "           'Content-type': 'audio/wav', 'codec': 'audio/pcm', 'samplerate': '16000'}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'scenarios': 'ulm',\n",
    "    'appid': 'D4D52672-91D7-4C74-8AD8-42B1D98141A5', # dont change, it is fixed by design\n",
    "    'locale': 'en-US', # speech in english\n",
    "    'device.os': 'PC',\n",
    "    'version': '3.0',\n",
    "    'format': 'json', # return value in json\n",
    "    'instanceid': str(uuid.uuid1()), # any guid\n",
    "    'requestid': str(uuid.uuid1()),\n",
    "})\n",
    "\n",
    "api_response = requests.post(url_stt_api, headers=headers, params=params, data=blob.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '3.0',\n",
       " 'header': {'status': 'success',\n",
       "  'scenario': 'ulm',\n",
       "  'name': 'for president for president president president',\n",
       "  'lexical': 'for president for president president president',\n",
       "  'properties': {'requestid': '99e4173f-0d7b-46cf-82d4-cb0750660c2d',\n",
       "   'HIGHCONF': '1'}},\n",
       " 'results': [{'scenario': 'ulm',\n",
       "   'name': 'for president for president president president',\n",
       "   'lexical': 'for president for president president president',\n",
       "   'confidence': '0.7758604',\n",
       "   'properties': {'HIGHCONF': '1'}}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "text = res_json['results'][0]['lexical']\n",
    "res_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a speech recognition request\n",
    "\n",
    "#try taking out token\n",
    "\n",
    "#request a token\n",
    "uri_api = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken'\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key': speech_recognition_key}\n",
    "response_api = requests.post(uri_api, headers=headers)\n",
    "token = str(response_api.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "\n",
    "def speech_recognition_request(blob_name, blob_container_name, speech_recognition_key):\n",
    "    \n",
    "    #load audio file to process\n",
    "    blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "    #send request to Speech Recognition service \n",
    "    url_api = 'https://speech.platform.bing.com/recognize'\n",
    "    headers = {'Authorization': 'Bearer {0}'.format(token),\n",
    "               'Content-type': 'audio/wav',\n",
    "               'codec': 'audio/pcm',\n",
    "               'samplerate': '16000'}\n",
    "    params = urllib.parse.urlencode({\n",
    "        'scenarios': 'ulm',\n",
    "        'appid': 'D4D52672-91D7-4C74-8AD8-42B1D98141A5',\n",
    "        'locale': 'en-US',\n",
    "        'device.os': 'PC',\n",
    "        'version': '3.0',\n",
    "        'format': 'json',\n",
    "        'instanceid': str(uuid.uuid1()),\n",
    "        'requestid': str(uuid.uuid1())})\n",
    "    response_api = requests.post(url_api, headers=headers, params=params, data=blob.content)\n",
    "\n",
    "    #from response\n",
    "    res_json = json.loads(response_api.content.decode('utf-8'))\n",
    "    text = res_json['results'][0]['lexical']\n",
    "    #confidence = res_json['results'][0]['confidence']\n",
    "    \n",
    "    return text #, confindence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrive your keys\n",
    "import pickle\n",
    "with open('../keys/azure_services_keys.json', 'rb') as handle:\n",
    "    azure_keys = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "\n",
    "video_text = []\n",
    "#text_confidence = []\n",
    "\n",
    "for blob_name in video_list:\n",
    "    text =speech_recognition_request(blob_name,'cloudcomputingcontainer',azure_keys['SPEECH_RECOGNITION']['API_KEY']) # confindence\n",
    "    video_text.append(text)\n",
    "    #text_confidence.append(confindence)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_keys['SPEECH_RECOGNITION']['API_KEY'] == azure_key_bing_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json['results'][0]['confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import library to display notebook as HTML\n",
    "import os\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "#path to .ccs style script\n",
    "cur_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "new_path = os.path.relpath('..\\\\..\\\\styles\\\\custom_styles_public_cloud_computing.css', cur_path)\n",
    "\n",
    "#function to display notebook\n",
    "def css():\n",
    "    style = open(new_path, \"r\").read()\n",
    "    return HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to apply HTML style\n",
    "css()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n",
    "\n",
    "## Microsoft Cognitive Services APIs with Python\n",
    "\n",
    "#### List of services shown in this tutorial:\n",
    "    \n",
    "- Caption Generation\n",
    "    - Face API to detect facial characteristics\n",
    "    - Computer Vision API to Analyze Image Contents\n",
    "    - Computer Vision API for Optical Character Recognition\n",
    "\n",
    "- Speech Recognition\n",
    "    - Bing Speech Recognition API\n",
    "\n",
    "- Text Analysis using Azure Machine Learning Studio Workshop\n",
    "    - Key Phrases\n",
    "    - Detect Language\n",
    "    - Sentiment Analysis\n",
    "    - Topic Detection\n",
    "\n",
    "\n",
    "- **what are some research applications?**\n",
    "---\n",
    "\n",
    "#### What are you going to learn\n",
    "\n",
    "- use cloud computing services to extract data from images and audio document\n",
    "- use microsoft azure cloud computing services\n",
    "- text analysis: predict and topic detection\n",
    "\n",
    "---\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- list necessary dependencies (pip install azure)\n",
    "- script to install dependecies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
