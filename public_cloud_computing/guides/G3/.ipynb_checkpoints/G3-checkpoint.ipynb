{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide 3: Public Cloud Computing with Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re probably using cloud computing right now, even if you don’t realize it. Any time you use an online service to send email, edit documents, play games, or store pictures and other files, it’s likely that cloud computing is making it all possible under the hood. In the previous guide we covered storage service and in this one we are going to show you how to use cloud computing services such as Computer Vision, Speech Recognition, and Text Analytics that will respectively allow us to extract features and contents from images, to transcribe audio files to text, and to perform text analysis with Microsoft Azure.\n",
    "Cloud computing services all work a little differently, depending on the provider. But many provide a friendly, browser-based dashboard that makes it easier for IT professionals and developers to order resources and manage their accounts. In the previous guides you familiarize with Azure Dashboard to create services and with Storage Explorer to manage your data. You also started  working with REST APIs and the SDK for Python that gave as more options for automating the processes. This guides is going to show you how to use the most interesting cloud computing services that we believe might be useful to social scientists for expanding the datasources to use for quantitative research.\n",
    "\n",
    "We'll go over the basics of Microsoft Azure Storage, but we should point out that a *lot* of talented people have given tutorials, and we won't do any better than they have. *TODO Point out some resources and explain why they are good (add links)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Guide 3: Public Cloud Computing with Microsoft Azure](#Guide-2:-Public-Cloud-Computing-with-Microsoft-Azure)\n",
    "* [Azure cloud computing basics](#Azure-Storage-Account-basics)\n",
    "* [Common Tasks in using Azure cloud computing services](#Common-Tasks-in-using-Azure-Storage-Account)\n",
    "    * [Use API and Request token](#Paragraph-3)\n",
    "* [Set up containers and upload files](#Set-up-containers-and-upload-files)\n",
    "* [Captions generation](#Captions-generation)\n",
    "    * [Demo 1: Face API to detect facial characteristics and Emotion](#Paragraph-3)\n",
    "    * [Demo 2: Computer Vision API to Analyze Image Contents](#Paragraph-3)\n",
    "    * [Demo 3: Computer Vision API for Optical Character Recognition](#Paragraph-3)\n",
    "* [Transcribe audio to text](#Transcribe-audio-to-text)\n",
    "    * [Demo 4: Bing Speech Recognition API to convert audio files to text](#Paragraph-3)\n",
    "* [Text Analysis](#Transcribe-audio-to-text)\n",
    "    * [Demo 5: Key Phrases](#Paragraph-3)\n",
    "    * [Demo 6: Sentiment Analysis](#Paragraph-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure cloud computing basics\n",
    "The first cloud computing services are barely a decade old, but already a variety of organizations—from tiny startups to global corporations, government agencies to non-profits—are embracing the technology for all sorts of reasons. Here are a few of the things you can do with the cloud:\n",
    "\n",
    "- Create new apps and services\n",
    "- Store, back up, and recover data\n",
    "- Host websites and blogs\n",
    "- Stream audio and video\n",
    "- Deliver software on demand\n",
    "- Analyze data for patterns and make predictions\n",
    "\n",
    "Here are 6 common reasons organizations are turning to cloud computing services:\n",
    "\n",
    "- **Cost**: eliminates/reduces a) the capital expense of buying hardware and software and setting up and running on-site datacenters, b) electricity for power and cooling, and c) the IT experts for managing the infrastructure.\n",
    "\n",
    "\n",
    "- **Cost**:2. Speed\n",
    "Most cloud computing services are provided self service and on demand, so even vast amounts of computing resources can be provisioned in minutes, typically with just a few mouse clicks, giving businesses a lot of flexibility and taking the pressure off capacity planning.\n",
    "\n",
    "\n",
    "- **Cost**:3. Global scale\n",
    "The benefits of cloud computing services include the ability to scale elastically. In cloud speak, that means delivering the right amount of IT resources—for example, more or less computing power, storage, bandwidth—right when its needed, and from the right geographic location.\n",
    "\n",
    "\n",
    "- **Cost**:4. Productivity\n",
    "On-site datacenters typically require a lot of “racking and stacking”—hardware set up, software patching, and other time-consuming IT management chores. Cloud computing removes the need for many of these tasks, so IT teams can spend time on achieving more important business goals.\n",
    "\n",
    "\n",
    "- **Cost**:5. Performance\n",
    "The biggest cloud computing services run on a worldwide network of secure datacenters, which are regularly upgraded to the latest generation of fast and efficient computing hardware. This offers several benefits over a single corporate datacenter, including reduced network latency for applications and greater economies of scale.\n",
    "\n",
    "\n",
    "- **Reliability**: makes data backup, disaster recovery, and business continuity easier and less expensive, because data can be mirrored at multiple redundant sites on the cloud provider’s network.\n",
    "\n",
    "## Common Tasks in using Azure Cloud Computing services\n",
    "### Use API and Request token\n",
    "https://www.soapui.org/learn/api/understanding-rest-headers-and-parameters.html\n",
    "\n",
    "## Set up containers, upload files and retrieve BLOB URLs\n",
    "\n",
    "- import functions from utilities.py \n",
    "- set directories to import images for demos (~\\public_cloud_computing\\data\\G3_demo)\n",
    "- retrieve storage account service credentials from your azure_keys (~\\public_cloud_computing\\guides\\keys)\n",
    "- create container, retrive files to download path and upload them. Use functions **`upload_files_to_container()`**\n",
    "- retieve BLOB names and URLs using the function **`retrieve_blob_list `**\n",
    "\n",
    "The function **`upload_files_to_container()`** call three functions at once for you: to retrieve files name, path and extensions uses **`get_files()`**, to create containers **`use make_public_container()`**, and to upload files to containers uses the function **`use upload_file()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import sys\n",
    "\n",
    "#import functions from utilities\n",
    "sys.path.insert(0, \"../utilities/\")\n",
    "try:\n",
    "    from utils import *\n",
    "except ImportError:\n",
    "    print('No Import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Your documents directories are:\n",
      "- notebook:\t C:\\Users\\popor\\iqss_workshop\\workshops\\public_cloud_computing\\guides\\G3\n",
      "- azure keys:\t C:\\Users\\popor\\iqss_workshop\\workshops\\public_cloud_computing\\guides\\keys\n",
      "- files:\t C:\\Users\\popor\\iqss_workshop\\workshops\\public_cloud_computing\\data\\G3_demo\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#set local directory to upload files from, and directory to import azure keys from\n",
    "cur_dir = os.getcwd()\n",
    "os.chdir('../keys/')\n",
    "dir_azure_keys = os.getcwd()\n",
    "os.chdir('../../data/G3_demo/')\n",
    "dir_files_demo = os.getcwd()\n",
    "\n",
    "#print directories\n",
    "print('---------------------------------------------------------')\n",
    "print('Your documents directories are:')\n",
    "print('- notebook:\\t', cur_dir)\n",
    "print('- azure keys:\\t', dir_azure_keys)\n",
    "print('- files:\\t', dir_files_demo)\n",
    "print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive keys\n",
    "\n",
    "#ERASE MY PATH BEFORE REALISING THE WORKSHOP MATERIALS\n",
    "my_path_to_keys = 'C:/Users/popor/Desktop/'\n",
    "\n",
    "#set service name, path to the keys and keys file name\n",
    "SERVICE_NAME = 'STORAGE' #add here: STORAGE, FACE, COMPUTER_VISION, SPEECH_RECOGNITION, TEXT_ANALYTICS\n",
    "PATH_TO_KEYS = my_path_to_keys #add here (use dir_azure_keys)\n",
    "KEYS_FILE_NAME = 'azure_services_keys_v1.1.json' #add file name (eg 'azure_services_keys.json')\n",
    "\n",
    "#call function to retrieve\n",
    "storage_keys = retrieve_keys(SERVICE_NAME, PATH_TO_KEYS, KEYS_FILE_NAME)\n",
    "\n",
    "#set storage name and keys\n",
    "STORAGE_NAME = storage_keys['NAME']\n",
    "STORAGE_KEY = storage_keys['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mycloudservices BLOB container has been successfully created: True\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Data stored from directory):\t C:\\Users\\popor\\iqss_workshop\\workshops\\public_cloud_computing\\data\\G3_demo\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Start uploading files\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "bill_clinton_hope_ad_1992_frame100.jpg // BLOB upload status: successful\n",
      "eisenhower_for_president_1952_frame600.jpg // BLOB upload status: successful\n",
      "high_quality_famous_daisy_attack_ad_from_1964_presidential_election_frame1400.jpg // BLOB upload status: successful\n",
      "humphrey_laughing_at_spiro_agnew_1968_political_ad_frame500.jpg // BLOB upload status: successful\n",
      "kennedy_for_me_campaign_jingle_jfk_1960_frame1000.jpg // BLOB upload status: successful\n",
      "kennedy_for_me_campaign_jingle_jfk_1960_frame200.jpg // BLOB upload status: successful\n",
      "mcgovern_defense_plan_ad_nixon_1972_presidential_campaign_commercial_frame200.jpg // BLOB upload status: successful\n",
      "ronald_reagan_tv_ad_its_morning_in_america_again_frame1200.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame1000.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame1100.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame300.jpg // BLOB upload status: successful\n",
      "1988_george_bush_sr_revolving_door_attack_ad_campaign_frame200.jpg // BLOB upload status: successful\n",
      "bill_clinton_hope_ad_1992_frame100.jpg // BLOB upload status: successful\n",
      "bill_clinton_hope_ad_1992_frame1200.jpg // BLOB upload status: successful\n",
      "eisenhower_for_president_1952_frame1500.jpg // BLOB upload status: successful\n",
      "eisenhower_for_president_1952_frame600.jpg // BLOB upload status: successful\n",
      "high_quality_famous_daisy_attack_ad_from_1964_presidential_election_frame1400.jpg // BLOB upload status: successful\n",
      "mcgovern_defense_plan_ad_nixon_1972_presidential_campaign_commercial_frame200.jpg // BLOB upload status: successful\n",
      "ronald_reagan_tv_ad_its_morning_in_america_again_frame100.jpg // BLOB upload status: successful\n",
      "ronald_reagan_tv_ad_its_morning_in_america_again_frame1200.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame1000.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame1100.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame200.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame300.jpg // BLOB upload status: successful\n",
      "yes_we_can__barack_obama_music_video_frame500.jpg // BLOB upload status: successful\n",
      "kennedy_for_me_campaign_jingle_jfk_1960_frame1000.jpg // BLOB upload status: successful\n",
      "kennedy_for_me_campaign_jingle_jfk_1960_frame200.jpg // BLOB upload status: successful\n",
      "eisenhower_for_president_1952_chunck_1.wav // BLOB upload status: successful\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Uploading completed\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "It took 7.75 seconds to upload 28 files\n"
     ]
    }
   ],
   "source": [
    "#set a name for a new container\n",
    "NEW_CONTAINER_NAME ='mycloudservices'\n",
    "\n",
    "#set the audio file directory\n",
    "DIR_FILES = dir_files_demo\n",
    "\n",
    "#set content type of the file, in this case is a audio .wav\n",
    "CONTENT_TYPE = 'image/'\n",
    "\n",
    "upload_files_to_container(STORAGE_NAME, STORAGE_KEY, NEW_CONTAINER_NAME, DIR_FILES, CONTENT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve BLOB names and URLs\n",
    "blob_name, blob_url = retrieve_blob_list(STORAGE_NAME, STORAGE_KEY, NEW_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/1988_george_bush_sr_revolving_door_attack_ad_campaign_frame200.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/1988_george_bush_sr_revolving_door_attack_ad_campaign_frame200.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/bill_clinton_hope_ad_1992_frame100.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/bill_clinton_hope_ad_1992_frame100.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/eisenhower_for_president_1952_frame600.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/eisenhower_for_president_1952_frame600.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/high_quality_famous_daisy_attack_ad_from_1964_presidential_election_frame1400.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/high_quality_famous_daisy_attack_ad_from_1964_presidential_election_frame1400.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/humphrey_laughing_at_spiro_agnew_1968_political_ad_frame500.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/humphrey_laughing_at_spiro_agnew_1968_political_ad_frame500.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/kennedy_for_me_campaign_jingle_jfk_1960_frame200.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/kennedy_for_me_campaign_jingle_jfk_1960_frame200.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/mcgovern_defense_plan_ad_nixon_1972_presidential_campaign_commercial_frame200.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/mcgovern_defense_plan_ad_nixon_1972_presidential_campaign_commercial_frame200.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/ronald_reagan_tv_ad_its_morning_in_america_again_frame1200.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/ronald_reagan_tv_ad_its_morning_in_america_again_frame1200.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame1100.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame1100.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame200.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame200.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame500.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame500.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import library to display image\n",
    "from IPython.display import Image as ipImage, display\n",
    "\n",
    "#display images \n",
    "for url, name in zip(blob_url, blob_name):\n",
    "    img = ipImage(url=url, width=200, height=200)\n",
    "    print(url)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captions generation\n",
    "### Demo 1: Detect Face Features and Emotion \n",
    "\n",
    "_**Description:**_\n",
    "Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes. Attributes include age, gender, headPose, smile, facialHair, glasses, emotion, hair, makeup, occlusion, accessories, blur, exposure and noise. It can detects up to 64 faces for an image. Faces are ranked by face rectangle size from large to small. Face detector prefer frontal and near-frontal faces. There are cases that faces may not be detected, e.g. exceptionally large face angles (head-pose) or being occluded, or wrong image orientation.\n",
    "\n",
    "_**Input/Output:**_\n",
    "JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n",
    "Faces are detectable when its size is 36x36 to 4096x4096 pixels. If need to detect very small but clear faces, please try to enlarge the input image. Higher face image quality means better detection and recognition precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n",
    "\n",
    "A succesfull response returns:\n",
    "![face_detect_output](img/face_detect_output.PNG)\n",
    "\n",
    "_**URI:**_ https://[cloud_service_location].api.cognitive.microsoft.com/face/v1.0/detect\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation ([FACE doc](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236))\n",
    "\n",
    "_**How to use the service using REST API:**\n",
    "- import libraries\n",
    "- retrieve service key\n",
    "- configure API access to request face service\n",
    "- set request headers\n",
    "- set request parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import requests\n",
    "import urllib\n",
    "#import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set service name\n",
    "SERVICE_NAME = 'FACE'\n",
    "\n",
    "#call function to retrive keys\n",
    "storage_keys = retrieve_keys(SERVICE_NAME, PATH_TO_KEYS, KEYS_FILE_NAME)\n",
    "\n",
    "#set text analytics keys\n",
    "FACE_KEY = storage_keys['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure API access to request face service\n",
    "URI_FACE = 'https://eastus.api.cognitive.microsoft.com/face/v1.0/detect'\n",
    "\n",
    "#set REST headers\n",
    "headers = {}\n",
    "headers['Content-Type'] = 'application/json'\n",
    "headers['Accept'] = 'application/json'\n",
    "headers['Ocp-Apim-Subscription-Key'] = FACE_KEY\n",
    "\n",
    "#set REST api request parameters\n",
    "params_set = {}\n",
    "params_set['returnFaceId'] = 'true'\n",
    "params_set['returnFaceLandmarks'] = 'true'\n",
    "params_set['returnFaceAttributes'] = 'gender,smile,age,emotion'\n",
    "\n",
    "# headers = {'Content-Type': 'application/octet-stream', # or 'application/json' for remote stored image files\n",
    "#            'Ocp-Apim-Subscription-Key': FACE_KEY} # for locally stored\n",
    "\n",
    "# # Set request parameters converted to HTTP protocol\n",
    "# # these parameters tell the api I want to detect a face and a smile\n",
    "# params = urllib.parse.urlencode({'returnFaceId' : 'true',\n",
    "#                                  'returnFaceLandmarks' : 'true',\n",
    "#                                  'returnFaceAttributes' : 'gender,smile,age,emotion'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes_we_can__barack_obama_music_video_frame1100.jpg had a <Response [200]> response\n"
     ]
    }
   ],
   "source": [
    "#set blob to use with FACE service\n",
    "BLOB_URL = 'https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame1100.jpg' #add here BLOB url\n",
    "\n",
    "#set a FACE request \n",
    "params = urllib.parse.urlencode(params_set)\n",
    "query_string = '?{0}'.format(params) \n",
    "url = URI_FACE + query_string\n",
    "body = '{\\'url\\':\\'' + BLOB_URL + '\\'}'\n",
    "\n",
    "#request for FACE service   \n",
    "api_response = requests.post(url, headers=headers, data=body)\n",
    "print('{} had a {} response'.format(BLOB_URL.split('/')[-1], api_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data from response in json format\n",
    "response = json.loads(api_response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_bytes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-82dc904c7b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCircle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'noseTip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'noseTip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'img_bytes' is not defined"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image size of 86429x71960 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'png'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'retina'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'png2x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[0;32m   2261\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2263\u001b[1;33m                 **kwargs)\n\u001b[0m\u001b[0;32m   2264\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mDraw\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[1;31m# acquire a lock on the shared font cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[0mRendererAgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[1;34m(self, cleared)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_new_renderer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\python\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, width, height, dpi)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Image size of 86429x71960 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in response:\n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "\n",
    "    pc = 'red' # patch color\n",
    "    if i['faceAttributes']['gender'] == 'male':\n",
    "        pc = 'blue'\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    ax.text(fr['left'], fr['top']+fr['height'], \n",
    "            'age:'+str(i['faceAttributes']['age']), \n",
    "            fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "\n",
    "ps = 5       # patch size\n",
    "pc = '#00FF00'   # patch color\n",
    "\n",
    "#Draw eye, nose, mouth\n",
    "for i in res_json:\n",
    "    fl = i['faceLandmarks']\n",
    "\n",
    "    # left eye\n",
    "    ax.add_patch(patches.Circle((fl['pupilLeft']['x'], fl['pupilLeft']['y']), ps, color=pc))\n",
    "\n",
    "    # right eye\n",
    "    ax.add_patch(patches.Circle((fl['pupilRight']['x'], fl['pupilRight']['y']), ps, color=pc))\n",
    "\n",
    "    # mouth\n",
    "    ax.add_patch(patches.Circle((fl['mouthLeft']['x'], fl['mouthLeft']['y']), ps, color=pc))\n",
    "    ax.add_patch(patches.Circle((fl['mouthRight']['x'], fl['mouthRight']['y']), ps, color=pc))\n",
    "\n",
    "    # nose\n",
    "    ax.add_patch(patches.Circle((fl['noseTip']['x'], fl['noseTip']['y']), ps, color=pc))\n",
    "\n",
    "plt.imshow(img_bytes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Start Face\n",
      "-------------------\n",
      "1988_george_bush_sr_revolving_door_attack_ad_campaign_frame200.jpg had a <Response [200]> response\n",
      "bill_clinton_hope_ad_1992_frame100.jpg had a <Response [200]> response\n",
      "eisenhower_for_president_1952_frame600.jpg had a <Response [200]> response\n",
      "high_quality_famous_daisy_attack_ad_from_1964_presidential_election_frame1400.jpg had a <Response [200]> response\n",
      "humphrey_laughing_at_spiro_agnew_1968_political_ad_frame500.jpg had a <Response [200]> response\n",
      "kennedy_for_me_campaign_jingle_jfk_1960_frame200.jpg had a <Response [200]> response\n",
      "mcgovern_defense_plan_ad_nixon_1972_presidential_campaign_commercial_frame200.jpg had a <Response [200]> response\n",
      "ronald_reagan_tv_ad_its_morning_in_america_again_frame1200.jpg had a <Response [200]> response\n",
      "yes_we_can__barack_obama_music_video_frame1100.jpg had a <Response [200]> response\n",
      "yes_we_can__barack_obama_music_video_frame200.jpg had a <Response [200]> response\n",
      "yes_we_can__barack_obama_music_video_frame500.jpg had a <Response [200]> response\n",
      "-------------------\n",
      "Conversion completed\n",
      "-------------------\n",
      "It took 2.84 seconds to \n"
     ]
    }
   ],
   "source": [
    "#store http response and json file\n",
    "responses = []\n",
    "http_responses = []\n",
    "\n",
    "#set procedure starting time\n",
    "print('-------------------')\n",
    "print(\"Start Face\")\n",
    "print('-------------------')\n",
    "start = time.time()\n",
    "\n",
    "#run FACE service on frames\n",
    "for blob in blob_url:\n",
    "    if blob.split('.')[-1] == 'jpg':\n",
    "        \n",
    "        #set a FACE request \n",
    "        params = urllib.parse.urlencode(params_set)\n",
    "        query_string = '?{0}'.format(params) \n",
    "        url = URI_FACE + query_string\n",
    "        body = '{\\'url\\':\\'' + blob + '\\'}'\n",
    "                \n",
    "        #request for FACE service   \n",
    "        api_response = requests.post(url, headers=headers, data=body)\n",
    "        print('{} had a {} response'.format(blob.split('/')[-1], api_response))\n",
    "\n",
    "        #extract data from response\n",
    "        res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "        http_responses.append(api_response)\n",
    "        responses.append(res_json)\n",
    "\n",
    "#load output next cell       \n",
    "        \n",
    "#set procedure ending time\n",
    "end = time.time()\n",
    "print('-------------------')\n",
    "print('Conversion completed')\n",
    "print('-------------------')\n",
    "print('It took {} seconds to '.format(round(end - start, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cloudcomputingplayground.blob.core.windows.net/mycloudservices/yes_we_can__barack_obama_music_video_frame500.jpg'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "image 0 h has no face\n",
      "t\n",
      "image 1 t has no face\n",
      "t\n",
      "image 2 t has no face\n",
      "p\n",
      "image 3 p has no face\n",
      "s\n",
      "image 4 s has no face\n",
      ":\n",
      "image 5 : has no face\n",
      "/\n",
      "image 6 / has no face\n",
      "/\n",
      "image 7 / has no face\n",
      "c\n",
      "l\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "for i, [response, name] in enumerate(zip(responses, blob_name)):\n",
    "    \n",
    "    print(name)\n",
    "    try:\n",
    "        response = response[0]\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        print('image {0} {1} has no face'.format(i,name))\n",
    "                                     \n",
    "#    if response[0]\n",
    "#    print(i,type(response[0]))\n",
    "#     if response is not empty:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set service name\n",
    "SERVICE_NAME = 'COMPUTER_VISION' #add here: STORAGE, FACE, COMPUTER_VISION, SPEECH_RECOGNITION, TEXT_ANALYTICS, ML_STUDIO\n",
    "\n",
    "#call function to retrive keys\n",
    "storage_keys = retrive_keys(SERVICE_NAME, PATH_TO_KEYS, KEYS_FILE_NAME)\n",
    "\n",
    "#set text analytics keys\n",
    "COMPUTER_VISION_KEY = storage_keys['API_KEY']\n",
    "\n",
    "#configure API access to request text analytics service\n",
    "URI_ANALYZE = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/analyze'\n",
    "URI_OCR = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr'\n",
    "\n",
    "#set REST headers\n",
    "headers = {}\n",
    "headers['Ocp-Apim-Subscription-Key'] = COMPUTER_VISION_KEY\n",
    "headers['Content-Type'] = 'application/json'\n",
    "headers['Accept'] = 'application/json'\n",
    "\n",
    "#set api request parameters\n",
    "params_set = {}\n",
    "params_set['visualFeatures'] = 'Categories,Tags,Description,Faces,ImageType,Color,Adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#What could we use the output from FACE API for?\n",
    "#\n",
    "#Research example:\n",
    "#\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captions generation\n",
    "### Demo 1: Detect Face Features and Emotion\n",
    "### Demo 2: Analyze Image\n",
    "### Demo 3: Optical Character Recognition\n",
    "\n",
    "## Transcribe audio to text\n",
    "### Demo 4: Speech Recognition\n",
    "\n",
    "## Text Analysis\n",
    "### Demo 5: Key Phrases\n",
    "### Demo 6: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to display image\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE API service Url (change based on the region*)\n",
    "face_api = 'https://eastus.api.cognitive.microsoft.com/face/v1.0/detect'\n",
    "# Azure Cognitive Services Key for FACE API\n",
    "key_1 = '620facc4717d4136b665e43a618c2911'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/leonid_andreev.png'    \n",
    "img = Image(url=img_url, width=300, height=350)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json', \n",
    "           'Ocp-Apim-Subscription-Key':key_1}\n",
    "\n",
    "params = urllib.parse.urlencode({'returnFaceId' : 'true',\n",
    "                                 'returnFaceLandmarks' : 'true',\n",
    "                                 'returnFaceAttributes' : 'age,gender,emotion,smile,hair,exposure'})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = face_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image's name to read\n",
    "blob_name = 'steven_worthington.png'\n",
    "\n",
    "# call method (i.e. get_blob_to_bytes to) read the images as byte array\n",
    "blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "# set variable that store in memory image's bytes \n",
    "blob_bytes_in_memory = io.BytesIO(blob.content)\n",
    "\n",
    "# read bytes and display image in the notebook\n",
    "img_bytes = Image.open(blob_bytes_in_memory)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(img_bytes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to send HTTP requests and to craft URL\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json', \n",
    "           'Ocp-Apim-Subscription-Key':key_1}\n",
    "\n",
    "params = urllib.parse.urlencode({'returnFaceId' : 'true',\n",
    "                                 'returnFaceLandmarks' : 'true',\n",
    "                                 'returnFaceAttributes' : 'age,gender,emotion,smile,hair,exposure'})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = face_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the request headers\n",
    "headers = {'Content-Type': 'application/octer-stream', # or 'application/json' for remote stored image files\n",
    "           'Ocp-Apim-Subscription-Key': key_1} # for locally stored\n",
    "\n",
    "# Set request parameters converted to HTTP protocol\n",
    "# these parameters tell the api I want to detect a face and a smile\n",
    "params = urllib.parse.urlencode({'returnFaceId' : 'true',\n",
    "                                 'returnFaceLandmarks' : 'true',\n",
    "                                 'returnFaceAttributes' : 'age,gender,emotion,smile,hair,exposure'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compone query with chosen parameters and associaed URL\n",
    "query = '?{0}'.format(params)\n",
    "url = face_api + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE api only returns 1 analysis at time\n",
    "api_response = requests.post(url, \n",
    "                             headers=headers, \n",
    "                             data=blob.content) # use local memory (use content of blob converted to array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How ML helps finding faces?\n",
    "\n",
    "##################################################################\n",
    "#                                                                #\n",
    "#                                                                #\n",
    "#                                                                #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in res_json:\n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "\n",
    "    pc = 'red' # patch color\n",
    "    if i['faceAttributes']['gender'] == 'male':\n",
    "        pc = 'blue'\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    ax.text(fr['left'], fr['top']+fr['height'], \n",
    "            'age:'+str(i['faceAttributes']['age']), \n",
    "            fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "\n",
    "# ps = 5       # patch size\n",
    "# pc = '#00FF00'   # patch color\n",
    "\n",
    "# #Draw eye, nose, mouth\n",
    "# for i in res_json:\n",
    "#     fl = i['faceLandmarks']\n",
    "\n",
    "#     # left eye\n",
    "#     ax.add_patch(patches.Circle((fl['pupilLeft']['x'], fl['pupilLeft']['y']), ps, color=pc))\n",
    "\n",
    "#     # right eye\n",
    "#     ax.add_patch(patches.Circle((fl['pupilRight']['x'], fl['pupilRight']['y']), ps, color=pc))\n",
    "\n",
    "#     # mouth\n",
    "#     ax.add_patch(patches.Circle((fl['mouthLeft']['x'], fl['mouthLeft']['y']), ps, color=pc))\n",
    "#     ax.add_patch(patches.Circle((fl['mouthRight']['x'], fl['mouthRight']['y']), ps, color=pc))\n",
    "\n",
    "#     # nose\n",
    "#     ax.add_patch(patches.Circle((fl['noseTip']['x'], fl['noseTip']['y']), ps, color=pc))\n",
    "\n",
    "plt.imshow(img_bytes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2) Analyze Image (vision/v1.0/analyze)\n",
    "\n",
    "\n",
    "_**Description:**_\n",
    "- Blah,blah\n",
    "\n",
    "_**Input/Output:**_\n",
    "- This, and that\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation (show the example on the user, then on the tutorial): https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa\n",
    "\n",
    "**##TODO** add link in pythonic, format (code style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library to display images\n",
    "from IPython.display import Image as ipImage, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process:\n",
    "#1_Set_Cognitive_Service_Key(e.g. Face, Computer Vision, etc)\n",
    "#2_Set_Service_API(eg. from FACE choose DETECT, from COMPUTER VISION choose ANALYZE)\n",
    "#3_BlOB\n",
    "#4_Display_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cognitive Services Key for COMPUTER VISION API\n",
    "azure_key_computer_vision = 'b9fb47b349694da0bb42ffb5d687c7c0'\n",
    "\n",
    "# Set API for Analyze service\n",
    "analyze_api = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/analyze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "img_url = 'https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/vision1.jpeg'    \n",
    "img = ipImage(url=img_url, width=450, height=450)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json', \n",
    "           'Ocp-Apim-Subscription-Key':azure_key_computer_vision}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'visualFeatures': 'Categories,Tags,Description,Faces,ImageType,Color,Adult',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = analyze_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**##TODO** \n",
    " - find more interesting pictures\n",
    " - display nice output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_Demo 3) Optical Character Recognition (vision/v1.0/ocr)_**\n",
    "\n",
    "\n",
    "_**Description:**_\n",
    "- Blah,blah\n",
    "\n",
    "_**Input/Output:**_\n",
    "- This, and that\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation (show the example on the user, then on the tutorial): \n",
    "https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fc\n",
    "\n",
    "**##TODO** add link in pythonic, format (code style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cognitive Services Key for COMPUTER VISION API\n",
    "# azure_key_computer_vision = 'b9fb47b349694da0bb42ffb5d687c7c0' (do not need this again)\n",
    "\n",
    "# Set API for OCR service\n",
    "ocr_api = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "# blob_name = 'ocr_iqss_1.png'\n",
    "# blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "# image_file_in_mem = io.BytesIO(blob)\n",
    "# img_bytes = Image.open(image_file_in_mem)\n",
    "\n",
    "# set image's name to read\n",
    "blob_name = 'ocr_iqss_1.png'\n",
    "\n",
    "# call method (i.e. get_blob_to_bytes to) read the images as byte array\n",
    "blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "# set variable that store in memory image's bytes \n",
    "blob_bytes_in_memory = io.BytesIO(blob.content)\n",
    "\n",
    "# read bytes and display image in the notebook\n",
    "img_bytes = Image.open(blob_bytes_in_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "# img_url = 'https://cloudcomputingplayground.blob.core.windows.net/cloudcomputingcontainer/ocr_iqss_1.png'\n",
    "# img = ipImage(url=img_url, width=500, height=500)\n",
    "# display(img)\n",
    "\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_api = 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr'\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', \n",
    "           'Ocp-Apim-Subscription-Key':azure_key_computer_vision}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'language': 'unk',\n",
    "    'detectOrientation': 'true',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = ocr_api + query_string\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "text_angle = 0\n",
    "try:\n",
    "    text_angle = res_json['textAngle']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Draw OCR rectangles\n",
    "for i in res_json['regions']:\n",
    "    pl = i['lines']\n",
    "    for k in pl:\n",
    "        words = k['words']\n",
    "        for l in words:\n",
    "            bb = l['boundingBox']\n",
    "            txt = l['text']\n",
    "            \n",
    "            bb = list(map(int, bb.split(',')))\n",
    "            \n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (bb[0], bb[1]), bb[2], bb[3], angle=text_angle,\n",
    "                    fill=False, linewidth=4, color='#00FF00')\n",
    "            )\n",
    "            \n",
    "            ax.text(bb[0], bb[1], txt, \n",
    "                    fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_Demo 4) Speech Recognition (sts/v1.0/issueToken)_**\n",
    "\n",
    "\n",
    "_**Description:**_\n",
    "- Blah,blah\n",
    "\n",
    "_**Input/Output:**_\n",
    "- This, and that\n",
    "\n",
    "_**Documentation:**_\n",
    "- Link to documentation (show the example on the user, then on the tutorial): \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/speech/getstarted/getstartedrest?tabs=Powershell\n",
    "\n",
    "**##TODO** \n",
    "add link in pythonic, \n",
    "format (code style), \n",
    "add picture to explain how it works(if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an example simple and build the more complex\n",
    "\n",
    "azure_key_bing_speech = '5d14da803af54010ba390d560b203354'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import requests\n",
    "import urllib\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = ['Eisenhower_1952.chunk0.wav','Eisenhower_1952.chunk1.wav'\n",
    "             'Eisenhower_1952.chunk2.wav','Eisenhower_1952.chunk3.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load speech file to process\n",
    "blob_name = 'Eisenhower_1952.chunk0.wav'\n",
    "blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "wav_bytes = Audio(data=blob.content)\n",
    "display(wav_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_token = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken'\n",
    "\n",
    "headers = {'Content-Length': '0', \n",
    "           'Ocp-Apim-Subscription-Key': azure_key_bing_speech}\n",
    "\n",
    "api_response = requests.post(uri_token, headers=headers)\n",
    "\n",
    "access_token = str(api_response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.content[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service\n",
    "# Call Speech to text service\n",
    "url_stt_api = 'https://speech.platform.bing.com/recognize' # service address \n",
    "\n",
    "headers = {\n",
    "           'Authorization': 'Bearer {0}'.format(access_token),\n",
    "           'Content-type': 'audio/wav', 'codec': 'audio/pcm', 'samplerate': '16000'}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'scenarios': 'ulm',\n",
    "    'appid': 'D4D52672-91D7-4C74-8AD8-42B1D98141A5', # dont change, it is fixed by design\n",
    "    'locale': 'en-US', # speech in english\n",
    "    'device.os': 'PC',\n",
    "    'version': '3.0',\n",
    "    'format': 'json', # return value in json\n",
    "    'instanceid': str(uuid.uuid1()), # any guid\n",
    "    'requestid': str(uuid.uuid1()),\n",
    "})\n",
    "\n",
    "api_response = requests.post(url_stt_api, headers=headers, params=params, data=blob.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "text = res_json['results'][0]['lexical']\n",
    "res_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a speech recognition request\n",
    "\n",
    "#try taking out token\n",
    "\n",
    "#request a token\n",
    "uri_api = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken'\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key': speech_recognition_key}\n",
    "response_api = requests.post(uri_api, headers=headers)\n",
    "token = str(response_api.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "\n",
    "def speech_recognition_request(blob_name, blob_container_name, speech_recognition_key):\n",
    "    \n",
    "    #load audio file to process\n",
    "    blob = blob_service.get_blob_to_bytes(blob_container_name, blob_name)\n",
    "\n",
    "    #send request to Speech Recognition service \n",
    "    url_api = 'https://speech.platform.bing.com/recognize'\n",
    "    headers = {'Authorization': 'Bearer {0}'.format(token),\n",
    "               'Content-type': 'audio/wav',\n",
    "               'codec': 'audio/pcm',\n",
    "               'samplerate': '16000'}\n",
    "    params = urllib.parse.urlencode({\n",
    "        'scenarios': 'ulm',\n",
    "        'appid': 'D4D52672-91D7-4C74-8AD8-42B1D98141A5',\n",
    "        'locale': 'en-US',\n",
    "        'device.os': 'PC',\n",
    "        'version': '3.0',\n",
    "        'format': 'json',\n",
    "        'instanceid': str(uuid.uuid1()),\n",
    "        'requestid': str(uuid.uuid1())})\n",
    "    response_api = requests.post(url_api, headers=headers, params=params, data=blob.content)\n",
    "\n",
    "    #from response\n",
    "    res_json = json.loads(response_api.content.decode('utf-8'))\n",
    "    text = res_json['results'][0]['lexical']\n",
    "    #confidence = res_json['results'][0]['confidence']\n",
    "    \n",
    "    return text #, confindence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive your keys\n",
    "import pickle\n",
    "with open('../keys/azure_services_keys.json', 'rb') as handle:\n",
    "    azure_keys = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "\n",
    "video_text = []\n",
    "#text_confidence = []\n",
    "\n",
    "for blob_name in video_list:\n",
    "    text =speech_recognition_request(blob_name,'cloudcomputingcontainer',azure_keys['SPEECH_RECOGNITION']['API_KEY']) # confindence\n",
    "    video_text.append(text)\n",
    "    #text_confidence.append(confindence)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_keys['SPEECH_RECOGNITION']['API_KEY'] == azure_key_bing_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json['results'][0]['confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library to display notebook as HTML\n",
    "import os\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "#path to .ccs style script\n",
    "cur_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "new_path = os.path.relpath('..\\\\..\\\\styles\\\\custom_styles_public_cloud_computing.css', cur_path)\n",
    "\n",
    "#function to display notebook\n",
    "def css():\n",
    "    style = open(new_path, \"r\").read()\n",
    "    return HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to apply HTML style\n",
    "css()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n",
    "\n",
    "## Microsoft Cognitive Services APIs with Python\n",
    "\n",
    "#### List of services shown in this tutorial:\n",
    "    \n",
    "- Captions generation\n",
    "    - Face API to detect facial characteristics\n",
    "    - Computer Vision API to Analyze Image Contents\n",
    "    - Computer Vision API for Optical Character Recognition\n",
    "\n",
    "- Transcribe audio to text\n",
    "    - Bing Speech Recognition API\n",
    "\n",
    "- Text Analysis\n",
    "    - Key Phrases \n",
    "    - Sentiment Analysis\n",
    "\n",
    "\n",
    "- **what are some research applications?**\n",
    "---\n",
    "\n",
    "#### What are you going to learn\n",
    "\n",
    "- use cloud computing services to extract data from images and audio document\n",
    "- use microsoft azure cloud computing services\n",
    "- text analysis: predict and topic detection\n",
    "\n",
    "---\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- list necessary dependencies (pip install azure)\n",
    "- script to install dependecies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add CSS file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
